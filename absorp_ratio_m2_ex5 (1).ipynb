{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Project: Trading Strategy based on PCA\n",
    "\n",
    "Welcome to your course project. This exercise gives you a hands-on experience to use PCA to:\n",
    "\n",
    "- construct eigen-portfolios\n",
    "- implement a measure of market systemic risk\n",
    "- develop simple trading strategy\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
    "- Do not modify the (# GRADED FUNCTION [function name]) comment in some cells. Your work would not be graded if you change this. Each cell containing that comment should only contain one function.\n",
    "- After coding your function, run the cell right below it to check if your result is correct.\n",
    "\n",
    "**After this assignment you will:**\n",
    "- Be able to use PCA to construct eigen-portfolios\n",
    "- Be able to use PCA to calculate a measure of market systemic risk\n",
    "- Be able to implement and analyze performance of portfolio strategy\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run Cell\" (denoted by a play symbol) in the upper bar of the notebook. \n",
    "\n",
    "We will often specify \"(â‰ˆ X lines of code)\" in the comments to tell you about how much code you need to write. It is just a rough estimate, so don't feel bad if your code is longer or shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [pandas](http://pandas.pydata.org/) Python data analysis library\n",
    "- [pandas](http://scikit-learn.org/stable/) scikit-learn - machine learning in Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Versions:\n",
      "  scikit-learn: 0.18.2\n",
      "  tensorflow: 1.10.1\n",
      "  pandas: 0.19.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(\"  scikit-learn: %s\" % sklearn.__version__)\n",
    "print(\"  tensorflow: %s\" % tf.__version__)\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import grading\n",
    "\n",
    "try:\n",
    "    import sklearn.model_selection\n",
    "    import sklearn.linear_model\n",
    "except:\n",
    "    print(\"Looks like an older version of sklearn package\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"  pandas: %s\"% pd.__version__)\n",
    "except:\n",
    "    print(\"Missing pandas package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ONLY FOR GRADING. DO NOT EDIT ###\n",
    "submissions=dict()\n",
    "assignment_key=\"LztgGBBtEeiaYgrsftMrjA\" \n",
    "all_parts=[\"oZXnf\", \"ahjZa\", \"9tUbW\",\"wjLiO\"]\n",
    "### ONLY FOR GRADING. DO NOT EDIT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COURSERA_TOKEN = # the key provided to the Student under his/her email on submission page\n",
    "# COURSERA_EMAIL = # the email\n",
    "COURSERA_TOKEN=\"3pTHafVtVDAshog3\"\n",
    "COURSERA_EMAIL=\"mohd_abdalla@hotmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset:  daily prices of  stocks from S&P 500 index  ####\n",
    "For this exercise we will be working with S&P 500 Index stock prices dataset. \n",
    "The following cell computes returns based for a subset of S&P 500 index stocks. It starts with stocks price data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset prices shape (3493, 419)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADSK</th>\n",
       "      <th>AEE</th>\n",
       "      <th>AEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-27</th>\n",
       "      <td>46.1112</td>\n",
       "      <td>78.9443</td>\n",
       "      <td>3.9286</td>\n",
       "      <td>4.5485</td>\n",
       "      <td>13.7898</td>\n",
       "      <td>15.6719</td>\n",
       "      <td>48.0313</td>\n",
       "      <td>10.8844</td>\n",
       "      <td>39.5477</td>\n",
       "      <td>8.1250</td>\n",
       "      <td>32.9375</td>\n",
       "      <td>33.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>45.8585</td>\n",
       "      <td>77.8245</td>\n",
       "      <td>3.6295</td>\n",
       "      <td>4.5485</td>\n",
       "      <td>14.2653</td>\n",
       "      <td>14.3906</td>\n",
       "      <td>47.7500</td>\n",
       "      <td>10.7143</td>\n",
       "      <td>38.5627</td>\n",
       "      <td>7.7188</td>\n",
       "      <td>32.3125</td>\n",
       "      <td>33.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31</th>\n",
       "      <td>44.5952</td>\n",
       "      <td>78.0345</td>\n",
       "      <td>3.7054</td>\n",
       "      <td>4.3968</td>\n",
       "      <td>14.5730</td>\n",
       "      <td>13.7656</td>\n",
       "      <td>46.7500</td>\n",
       "      <td>10.6576</td>\n",
       "      <td>37.3807</td>\n",
       "      <td>7.6406</td>\n",
       "      <td>32.5625</td>\n",
       "      <td>33.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-01</th>\n",
       "      <td>47.8377</td>\n",
       "      <td>80.7640</td>\n",
       "      <td>3.5804</td>\n",
       "      <td>4.5333</td>\n",
       "      <td>14.7128</td>\n",
       "      <td>13.9688</td>\n",
       "      <td>49.0000</td>\n",
       "      <td>10.8844</td>\n",
       "      <td>37.9717</td>\n",
       "      <td>7.9219</td>\n",
       "      <td>32.5625</td>\n",
       "      <td>33.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02</th>\n",
       "      <td>51.5434</td>\n",
       "      <td>83.4934</td>\n",
       "      <td>3.5290</td>\n",
       "      <td>4.5788</td>\n",
       "      <td>14.7968</td>\n",
       "      <td>15.3281</td>\n",
       "      <td>48.1250</td>\n",
       "      <td>10.6576</td>\n",
       "      <td>35.9032</td>\n",
       "      <td>7.9688</td>\n",
       "      <td>32.5625</td>\n",
       "      <td>33.6250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A       AA    AAPL     ABC      ABT     ADBE      ADI  \\\n",
       "2000-01-27  46.1112  78.9443  3.9286  4.5485  13.7898  15.6719  48.0313   \n",
       "2000-01-28  45.8585  77.8245  3.6295  4.5485  14.2653  14.3906  47.7500   \n",
       "2000-01-31  44.5952  78.0345  3.7054  4.3968  14.5730  13.7656  46.7500   \n",
       "2000-02-01  47.8377  80.7640  3.5804  4.5333  14.7128  13.9688  49.0000   \n",
       "2000-02-02  51.5434  83.4934  3.5290  4.5788  14.7968  15.3281  48.1250   \n",
       "\n",
       "                ADM      ADP    ADSK      AEE      AEP  \n",
       "2000-01-27  10.8844  39.5477  8.1250  32.9375  33.5625  \n",
       "2000-01-28  10.7143  38.5627  7.7188  32.3125  33.0000  \n",
       "2000-01-31  10.6576  37.3807  7.6406  32.5625  33.5000  \n",
       "2000-02-01  10.8844  37.9717  7.9219  32.5625  33.6875  \n",
       "2000-02-02  10.6576  35.9032  7.9688  32.5625  33.6250  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# load dataset\n",
    "\n",
    "asset_prices = pd.read_csv('/home/jovyan/work/readonly/spx_holdings_and_spx_closeprice.csv',\n",
    "                     date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),\n",
    "                     index_col = 0).dropna()\n",
    "n_stocks_show = 12\n",
    "print('Asset prices shape', asset_prices.shape)\n",
    "asset_prices.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate daily log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADSK</th>\n",
       "      <th>AEE</th>\n",
       "      <th>AEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.005480</td>\n",
       "      <td>-0.014185</td>\n",
       "      <td>-0.076134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034482</td>\n",
       "      <td>-0.081758</td>\n",
       "      <td>-0.005857</td>\n",
       "      <td>-0.015628</td>\n",
       "      <td>-0.024907</td>\n",
       "      <td>-0.049994</td>\n",
       "      <td>-0.018975</td>\n",
       "      <td>-0.016760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31</th>\n",
       "      <td>-0.027548</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>-0.033352</td>\n",
       "      <td>0.021570</td>\n",
       "      <td>-0.043431</td>\n",
       "      <td>-0.020942</td>\n",
       "      <td>-0.005292</td>\n",
       "      <td>-0.030651</td>\n",
       "      <td>-0.010131</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.015152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-01</th>\n",
       "      <td>0.072710</td>\n",
       "      <td>0.034978</td>\n",
       "      <td>-0.033735</td>\n",
       "      <td>0.031045</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>0.014761</td>\n",
       "      <td>0.048128</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.015810</td>\n",
       "      <td>0.036816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02</th>\n",
       "      <td>0.077464</td>\n",
       "      <td>0.033795</td>\n",
       "      <td>-0.014356</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.097310</td>\n",
       "      <td>-0.017857</td>\n",
       "      <td>-0.020837</td>\n",
       "      <td>-0.054475</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-03</th>\n",
       "      <td>0.016340</td>\n",
       "      <td>-0.031014</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>-0.006617</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.126402</td>\n",
       "      <td>0.098701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067217</td>\n",
       "      <td>0.035288</td>\n",
       "      <td>0.011516</td>\n",
       "      <td>0.033457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A        AA      AAPL       ABC       ABT      ADBE  \\\n",
       "2000-01-28 -0.005480 -0.014185 -0.076134  0.000000  0.034482 -0.081758   \n",
       "2000-01-31 -0.027548  0.002698  0.020912 -0.033352  0.021570 -0.043431   \n",
       "2000-02-01  0.072710  0.034978 -0.033735  0.031045  0.009593  0.014761   \n",
       "2000-02-02  0.077464  0.033795 -0.014356  0.010037  0.005709  0.097310   \n",
       "2000-02-03  0.016340 -0.031014  0.045537 -0.006617  0.005670  0.126402   \n",
       "\n",
       "                 ADI       ADM       ADP      ADSK       AEE       AEP  \n",
       "2000-01-28 -0.005857 -0.015628 -0.024907 -0.049994 -0.018975 -0.016760  \n",
       "2000-01-31 -0.020942 -0.005292 -0.030651 -0.010131  0.007737  0.015152  \n",
       "2000-02-01  0.048128  0.021281  0.015810  0.036816  0.000000  0.005597  \n",
       "2000-02-02 -0.017857 -0.020837 -0.054475  0.005920  0.000000 -0.001855  \n",
       "2000-02-03  0.098701  0.000000  0.067217  0.035288  0.011516  0.033457  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_returns = np.log(asset_prices) - np.log(asset_prices.shift(1))\n",
    "asset_returns = asset_prices.pct_change(periods=1)\n",
    "asset_returns = asset_returns.iloc[1:, :]\n",
    "asset_returns.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_returns(r_df):\n",
    "    \"\"\"\n",
    "    Normalize, i.e. center and divide by standard deviation raw asset returns data\n",
    "\n",
    "    Arguments:\n",
    "    r_df -- a pandas.DataFrame of asset returns\n",
    "\n",
    "    Return:\n",
    "    normed_df -- normalized returns\n",
    "    \"\"\"\n",
    "    mean_r = r_df.mean(axis=0)\n",
    "    sd_r = r_df.std(axis=0)\n",
    "    normed_df = (r_df - mean_r) / sd_r\n",
    "    return normed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADI</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADSK</th>\n",
       "      <th>AEE</th>\n",
       "      <th>AEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.190054</td>\n",
       "      <td>-0.513710</td>\n",
       "      <td>-2.714709</td>\n",
       "      <td>-0.049779</td>\n",
       "      <td>2.182933</td>\n",
       "      <td>-2.684131</td>\n",
       "      <td>-0.212461</td>\n",
       "      <td>-0.766996</td>\n",
       "      <td>-1.540731</td>\n",
       "      <td>-1.803947</td>\n",
       "      <td>-1.372991</td>\n",
       "      <td>-0.994169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31</th>\n",
       "      <td>-0.898232</td>\n",
       "      <td>0.096888</td>\n",
       "      <td>0.688156</td>\n",
       "      <td>-1.757230</td>\n",
       "      <td>1.355644</td>\n",
       "      <td>-1.438899</td>\n",
       "      <td>-0.720771</td>\n",
       "      <td>-0.279098</td>\n",
       "      <td>-1.891884</td>\n",
       "      <td>-0.391433</td>\n",
       "      <td>0.547298</td>\n",
       "      <td>0.871919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-01</th>\n",
       "      <td>2.319164</td>\n",
       "      <td>1.264327</td>\n",
       "      <td>-1.227995</td>\n",
       "      <td>1.539597</td>\n",
       "      <td>0.588289</td>\n",
       "      <td>0.451774</td>\n",
       "      <td>1.606541</td>\n",
       "      <td>0.975244</td>\n",
       "      <td>0.948126</td>\n",
       "      <td>1.272129</td>\n",
       "      <td>-0.008894</td>\n",
       "      <td>0.313197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02</th>\n",
       "      <td>2.471738</td>\n",
       "      <td>1.221529</td>\n",
       "      <td>-0.548494</td>\n",
       "      <td>0.464060</td>\n",
       "      <td>0.339454</td>\n",
       "      <td>3.133764</td>\n",
       "      <td>-0.616815</td>\n",
       "      <td>-1.012898</td>\n",
       "      <td>-3.348109</td>\n",
       "      <td>0.177340</td>\n",
       "      <td>-0.008894</td>\n",
       "      <td>-0.122594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-03</th>\n",
       "      <td>0.510174</td>\n",
       "      <td>-1.122380</td>\n",
       "      <td>1.551619</td>\n",
       "      <td>-0.388563</td>\n",
       "      <td>0.336944</td>\n",
       "      <td>4.078966</td>\n",
       "      <td>3.310577</td>\n",
       "      <td>-0.029293</td>\n",
       "      <td>4.090396</td>\n",
       "      <td>1.217955</td>\n",
       "      <td>0.818989</td>\n",
       "      <td>1.942390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A        AA      AAPL       ABC       ABT      ADBE  \\\n",
       "2000-01-28 -0.190054 -0.513710 -2.714709 -0.049779  2.182933 -2.684131   \n",
       "2000-01-31 -0.898232  0.096888  0.688156 -1.757230  1.355644 -1.438899   \n",
       "2000-02-01  2.319164  1.264327 -1.227995  1.539597  0.588289  0.451774   \n",
       "2000-02-02  2.471738  1.221529 -0.548494  0.464060  0.339454  3.133764   \n",
       "2000-02-03  0.510174 -1.122380  1.551619 -0.388563  0.336944  4.078966   \n",
       "\n",
       "                 ADI       ADM       ADP      ADSK       AEE       AEP  \n",
       "2000-01-28 -0.212461 -0.766996 -1.540731 -1.803947 -1.372991 -0.994169  \n",
       "2000-01-31 -0.720771 -0.279098 -1.891884 -0.391433  0.547298  0.871919  \n",
       "2000-02-01  1.606541  0.975244  0.948126  1.272129 -0.008894  0.313197  \n",
       "2000-02-02 -0.616815 -1.012898 -3.348109  0.177340 -0.008894 -0.122594  \n",
       "2000-02-03  3.310577 -0.029293  4.090396  1.217955  0.818989  1.942390  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_r = center_returns(asset_returns)\n",
    "normed_r.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to compute Absorption Ratio(AR). We do so by defining a moving look back window over which we collect returns for computing PCA. We start off from the earliest historical data and march forward moving by step_size, which we also choose arbitrary. For each such window we compute PCA and AR, fixing in advance number of components in the enumerator. Specifically, for we use the following hyper-parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 (Implement exponentially-weighted)\n",
    "\n",
    "**Instructions:**\n",
    "Implement exponent_weighting function which returns a sequence of $w_j$ as np.array. See below:\n",
    "\n",
    "Define sequence of $X_j$ where $j \\subset [N, 0]$, an integer taking all values in the interval from 0 to N  $$ X_j =  e^{-\\frac{log(2)}{H} \\times  \\space j}$$\n",
    "where H is half-life which determines the speed of decay, and $log$ is natural log function\n",
    "Then a sequence of exponentially decaying weights $w_j$ is defined as $$ w_j = \\frac{X_j}{ \\sum\\limits_{j=0}^N X_j } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: exponent_weighting# GRADE \n",
    "def exponent_weighting(n_periods, half_life = 252):\n",
    "    \"\"\"\n",
    "    Calculate exponentially (decaying) smoothed normalized (in probability density function sense) weights\n",
    "    Give more weight to recent observations while still considering the entire history of data\n",
    "    Make strategy more responsive to recent market changes, which can be crucial \n",
    "    for capturing short-term trends or shifts in market sentiment.\n",
    "    Compared to simple moving averages, exponentially smoothed averages respond \n",
    "    faster to new price information, reducing the lag and helping traders react \n",
    "    more promptly to market movements.\n",
    "    By applying exponentially decaying weights, the impact of older data points \n",
    "    is gradually reduced. This helps to smooth out noise and avoid overreacting \n",
    "    to short-term volatility that might not reflect true market trends.\n",
    "    Exponentially smoothed weights provide a balance between responsiveness to \n",
    "    new information and stability, making them well-suited for trading strategies. They help to reduce noise, capture trends more effectively, and allow for customization to align with specific trading goals.\n",
    "    By emphasizing recent data, the strategy can better capture short-term trends \n",
    "    and provide more reliable trading signals, leading to potentially better \n",
    "    trading performance.\n",
    "   \n",
    "    Arguments:\n",
    "    n_periods -- number of periods, an integer, N in the formula above\n",
    "    half_life -- half-life, which determines the speed of decay, h in the formula\n",
    "    The half-life is a crucial parameter in exponentially smoothed weights, controlling how quickly the influence of past data decays\n",
    "    \n",
    "    Return:\n",
    "    exp_probs -- exponentially smoothed weights, np.array\n",
    "    \"\"\"\n",
    "    \n",
    "    exp_probs = np.zeros(n_periods) # do your own calculation instead of dummy zero array\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    ### ...\n",
    "    x_j = np.exp( (-np.log(2) / half_life) * np.arange(n_periods))\n",
    "    X = np.sum(np.exp( (-np.log(2) / half_life) * np.arange(n_periods)))\n",
    "    exp_probs = x_j / X\n",
    "    \n",
    "    return exp_probs\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7be0c24ab4a8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQSEiIrlSUIiISK4UFCIikqv/D/nRDooFN9H+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7be0c259e240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Incorporating exponentially weighted probabilities in a PCA-based trading strategy\n",
    "enhances the model's responsiveness to recent market changes, reduces noise, and \n",
    "provides more stable and reliable signals. This approach ensures that the strategy\n",
    "adapts to current market conditions while still considering the overall historical\n",
    "context, leading to potentially better trading performance.\n",
    "'''\n",
    "exp_probs = exponent_weighting(252*1)\n",
    "plt.plot(exp_probs, linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong, please have a look at the reponse of the grader\n",
      "-------------------------\n",
      "{\"errorCode\":\"invalidEmailOrToken\",\"message\":\"Invalid email or token.\",\"details\":null}\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00549361,  0.00547852,  0.00546347,  0.00544846,  0.0054335 ,\n",
       "        0.00541857,  0.00540369,  0.00538885,  0.00537404,  0.00535928,\n",
       "        0.00534456,  0.00532988,  0.00531524,  0.00530064,  0.00528608,\n",
       "        0.00527156,  0.00525708,  0.00524264,  0.00522824,  0.00521388,\n",
       "        0.00519956,  0.00518528,  0.00517103,  0.00515683,  0.00514267,\n",
       "        0.00512854,  0.00511445,  0.0051004 ,  0.00508639,  0.00507242,\n",
       "        0.00505849,  0.00504459,  0.00503074,  0.00501692,  0.00500314,\n",
       "        0.0049894 ,  0.00497569,  0.00496202,  0.00494839,  0.0049348 ,\n",
       "        0.00492125,  0.00490773,  0.00489425,  0.00488081,  0.0048674 ,\n",
       "        0.00485403,  0.0048407 ,  0.0048274 ,  0.00481414,  0.00480092,\n",
       "        0.00478773,  0.00477458,  0.00476146,  0.00474838,  0.00473534,\n",
       "        0.00472233,  0.00470936,  0.00469643,  0.00468353,  0.00467066,\n",
       "        0.00465783,  0.00464504,  0.00463228,  0.00461956,  0.00460687,\n",
       "        0.00459421,  0.00458159,  0.00456901,  0.00455646,  0.00454394,\n",
       "        0.00453146,  0.00451901,  0.0045066 ,  0.00449422,  0.00448188,\n",
       "        0.00446957,  0.00445729,  0.00444505,  0.00443284,  0.00442066,\n",
       "        0.00440852,  0.00439641,  0.00438433,  0.00437229,  0.00436028,\n",
       "        0.0043483 ,  0.00433636,  0.00432445,  0.00431257,  0.00430072,\n",
       "        0.00428891,  0.00427713,  0.00426538,  0.00425367,  0.00424198,\n",
       "        0.00423033,  0.00421871,  0.00420712,  0.00419557,  0.00418404])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_1=list(exp_probs[:100])\n",
    "try:\n",
    "    part1 = \" \".join(map(repr, part_1))\n",
    "except TypeError:\n",
    "    part1 = repr(part_1)\n",
    "submissions[all_parts[0]]=part1\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:1],all_parts,submissions)\n",
    "exp_probs[:100]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def absorption_ratio(explained_variance, n_components):\n",
    "    \"\"\"\n",
    "    Calculate absorption ratio via PCA. absorption_ratio() is NOT to be used with Auto-Encoder. \n",
    "    The absorption ratio is used to determine the significance of principal \n",
    "    components. It helps in deciding how many principal components to retain \n",
    "    for constructing the portfolio\n",
    "    It serves as a quantitative measure to guide the selection of principal components, thereby \n",
    "    optimizing portfolio construction and enhancing the strategy's effectiveness \n",
    "    in capturing market trends.\n",
    "   \n",
    "    Arguments:\n",
    "    explained_variance -- 1D np.array of explained variance by each pricincipal component, in descending order\n",
    "    \n",
    "    n_components -- an integer, a number of principal components to compute absorption ratio\n",
    "    \n",
    "    Return:\n",
    "    ar -- absorption ratio\n",
    "    \"\"\"\n",
    "    ar = np.sum(explained_variance[:n_components]) / np.sum(explained_variance)\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (Implement Linear Auto-Encoder)\n",
    "\n",
    "LinearAutoEncoder class has two fully connected layers and no activation functions between the layers.\n",
    "\n",
    "**Instructions:**\n",
    "- fill missing code within LinearAutoEncoder class\n",
    "- in init() method of LinearAutoEncoder setup neural network\n",
    "    - **self.codings_layer** is a fully connected layer with **n_codings** neurons and no activation function\n",
    "    - **self.outputs** is a fully connected layer with **n_outputs** neurons and no activation function\n",
    "    - define loss function as Mean Square Error between the outputs and inputs referenced by **self.X** in the code\n",
    "    - use AdamOptimizer to optimize model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "class LinearAutoEncoder:\n",
    "    \"\"\"\n",
    "    To perform simple PCA, we set activation_fn=None \n",
    "    i.e., all neurons are linear and the cost function is the Mean-Square Error (MSE)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_codings, learning_rate=0.01):\n",
    "        # the learning rate in TensorFlow is a critical parameter that controls how quickly or slowly a model learns from the data\n",
    "        self.learning_rate = learning_rate\n",
    "        n_outputs = n_inputs\n",
    "        self.destroy() #cleans the TF\n",
    "        reset_graph()\n",
    "    \n",
    "        # the inputs are n_inputs x n_inputs covariance matrices\n",
    "        '''\n",
    "        This setup is foundational for many TensorFlow models, especially when dealing\n",
    "        with input data that has a fixed size but variable batch sizes (None) during training\n",
    "        or evaluation. Adjust n_inputs based on your specific data dimensions or model\n",
    "        requirements.\n",
    "        '''\n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, n_inputs, n_inputs])\n",
    "        '''\n",
    "         setup defines a basic linear autoencoder in TensorFlow where self.codings_layer \n",
    "         represents the compressed representation (encoding) of the input data (self.X),\n",
    "         and self.outputs represents the reconstructed output from the decoder part of \n",
    "         the autoencoder\n",
    "        '''\n",
    "        with tf.name_scope(\"lin_ae\"): # linear autoencoder\n",
    "            self.codings_layer = None #The coding layer is the output of the encoder part of the autoencoder. It represents the intermediate coding layer of the autoencoder\n",
    "            #  One of the primary purposes of the coding layer is to perform dimensionality reduction. By reducing the dimensionality of the input data, the autoencoder learns a more compact and efficient representation that can still accurately reconstruct the original data\n",
    "            self.outputs = None # represents the reconstructed outputs of the autoencoder\n",
    "            ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "            self.codings_layer = fully_connected(self.X, n_codings, activation_fn=None) #  dense layers\n",
    "            self.outputs = fully_connected(self.codings_layer, n_outputs, activation_fn=None) # sets up the output layer of a neural network\n",
    "            ### END CODE HERE ###\n",
    "        \n",
    "        with tf.name_scope(\"loss\"): #calculate the loss fn\n",
    "            self.reconstruction_loss = None\n",
    "            self.training_op = None\n",
    "            ### START CODE HERE ### (â‰ˆ 4-5 lines of code)\n",
    "            self.reconstruction_loss = tf.reduce_mean(tf.squared_difference(self.X, self.outputs)) # computes the mean squared error (MSE) between self.X (input data) and self.outputs (predicted data\n",
    "            # initializes an Adam optimizer with a specified learning rate (learning_rate) and applies gradient descent to minimize the reconstruction loss during training\n",
    "            self.training_op = tf.train.AdamOptimizer(learning_rate).minimize(self.reconstruction_loss) \n",
    "            ### END CODE HERE ###\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            '''\n",
    "            This operation (self.init) initializes all global variables in the TensorFlow \n",
    "            graph. Itâ€™s necessary to run this operation before training your model to \n",
    "            initialize variables like weights and biases in the neural network.\n",
    "            '''\n",
    "            \n",
    "    def destroy(self):\n",
    "        if hasattr(self, 'sess') and self.sess is not None:\n",
    "            self.sess.close()\n",
    "            self.sess = None\n",
    "\n",
    "    def absorption_ratio(self, test_input):\n",
    "        \"\"\"\n",
    "        Calculate absorption ratio based on already trained model\n",
    "        \"\"\"\n",
    "        if self.outputs is None:\n",
    "            return test_input, 0.\n",
    "        \n",
    "        with self.sess.as_default():  # do not close session\n",
    "            codings = self.codings_layer.eval(feed_dict={self.X: test_input})\n",
    "\n",
    "            # calculate variance explained ratio\n",
    "            result_ = self.outputs.eval(feed_dict={self.X: test_input})\n",
    "            var_explained = np.sum(np.diag(result_.squeeze())) / np.sum(np.diag(test_input.squeeze()))\n",
    "\n",
    "        return codings[0, :, :], var_explained\n",
    "    \n",
    "    def next_batch(self, X_train, batch_size):\n",
    "        \"\"\"\n",
    "        X_train - np.array of double of size K x N x N, where N is dimensionality of the covariance matrix\n",
    "        batch_size - an integer, number of training examples to feed through the nwtwork at once\n",
    "        \"\"\"\n",
    "        y_batch = None\n",
    "\n",
    "        selected_idx = np.random.choice(tuple(range(X_train.shape[0])), size=batch_size)\n",
    "        X_batch = X_train[selected_idx, :, :]\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def train(self, X_train, X_test, n_epochs=5, batch_size=2, verbose=False):\n",
    "        \"\"\"\n",
    "        train simple auto-encoder network\n",
    "        :param X_train:\n",
    "        :param X_test:\n",
    "        :param n_epochs: number of epochs to use for training the model\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.outputs is None:\n",
    "            return X_test, 0.\n",
    "        \n",
    "        n_examples = len(X_train)  # number of training examples\n",
    "        self.sess = tf.Session() #initializes a TensorFlow session \n",
    "        \n",
    "        # as_default context manager does not close the session when you exit the context,\n",
    "        # and you must close the session explicitly.\n",
    "        with self.sess.as_default():\n",
    "            self.init.run()\n",
    "            #  training loop for an autoencoder model, along with a monitoring mechanism \n",
    "            # to evaluate and print the reconstruction loss (MSE) for both training and \n",
    "            # test datasets after each epoch.\n",
    "            '''\n",
    "            Definition: An epoch refers to a complete pass over the entire training\n",
    "            dataset. During each epoch, the model sees all available training examples\n",
    "            in batches, updating its parameters based on the cumulative gradients \n",
    "            computed from each batch.\n",
    "\n",
    "            Iterative Improvement: Multiple epochs allow the model to iteratively \n",
    "            improve its ability to reconstruct input data. By seeing the data multiple \n",
    "            times in different batches and updating parameters, the model refines its \n",
    "            learned representations.\n",
    "            '''\n",
    "            for epoch in range(n_epochs):\n",
    "                n_batches = n_examples // min(n_examples, batch_size) #  calculates the number of batches\n",
    "                # The training for n_batches times\n",
    "                '''\n",
    "                Purpose: The training loop iterates over batches of data during the training \n",
    "                process. Each iteration updates the model's parameters (weights and biases) \n",
    "                based on the loss computed from the reconstructed output compared to the \n",
    "                original input.\n",
    "                '''\n",
    "                for _ in range(n_batches):\n",
    "                    X_batch, y_batch = self.next_batch(X_train, batch_size)\n",
    "                    '''\n",
    "                    Executes the training operation (self.training_op) within the TensorFlow \n",
    "                    session (self.sess). It feeds the current batch (X_batch) into the placeholder \n",
    "                    self.X using the feed_dict argument. This operation triggers the optimization \n",
    "                    process defined earlier to minimize the reconstruction loss or another defined\n",
    "                    loss function\n",
    "                    '''\n",
    "                    self.sess.run(self.training_op, feed_dict={self.X: X_batch})\n",
    "                \n",
    "                '''\n",
    "                A training monitoring mechanism for the autoencoder, where the reconstruction \n",
    "                loss (self.reconstruction_loss) is evaluated and printed for both training \n",
    "                and test datasets at the end of each epoch\n",
    "                '''\n",
    "                if verbose:\n",
    "                    # last covariance matrix from the training sample\n",
    "                    if X_train.shape[0] == 1:\n",
    "                        mse_train = self.reconstruction_loss.eval(feed_dict={self.X: X_train})\n",
    "                    else:\n",
    "                        mse_train = self.reconstruction_loss.eval(feed_dict={self.X: np.array([X_train[-1, :, :]])})\n",
    "                    mse_test = self.reconstruction_loss.eval(feed_dict={self.X: X_test})\n",
    "                    print('Epoch %d. MSE Train %.4f, MSE Test %.4f' % (epoch, mse_train, mse_test))\n",
    "\n",
    "            # calculate variance explained ratio\n",
    "            test_input = np.array([X_train[-1, :, :]])\n",
    "            result_ = self.outputs.eval(feed_dict={self.X: test_input})\n",
    "            var_explained = np.sum(np.diag(result_.squeeze())) / np.sum(np.diag(test_input.squeeze()))\n",
    "            print('Linear Auto-Encoder: variance explained: %.2f' % var_explained)\n",
    "            '''\n",
    "            retrieves the encoded representations (codings) of the X_test \n",
    "            dataset using the trained autoencoder mode\n",
    "            '''\n",
    "            codings = self.codings_layer.eval(feed_dict={self.X: X_test})\n",
    "            print('Done training linear auto-encoder')\n",
    "\n",
    "        return codings[0, :, :], var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time index and covariance matrix shape 1000 (504, 418)\n",
      "time index and covariance matrix shape 1060 (504, 418)\n",
      "time index and covariance matrix shape 1120 (504, 418)\n",
      "time index and covariance matrix shape 1180 (504, 418)\n",
      "time index and covariance matrix shape 1240 (504, 418)\n",
      "Done\n",
      "Linear Auto-Encoder: variance explained: 0.39\n",
      "Done training linear auto-encoder\n"
     ]
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "ix_offset = 1000\n",
    "stock_tickers = asset_returns.columns.values[:-1]\n",
    "assert 'SPX' not in stock_tickers, \"By accident included SPX index\"\n",
    "\n",
    "step_size = 60\n",
    "num_samples = 5\n",
    "lookback_window = 252 * 2   # in (days)\n",
    "num_assets = len(stock_tickers)\n",
    "cov_matricies = np.zeros((num_samples, num_assets, num_assets)) # hold training data\n",
    "\n",
    "# collect training and test data\n",
    "ik = 0\n",
    "for ix in range(ix_offset, min(ix_offset + num_samples * step_size, len(normed_r)), step_size):\n",
    "    ret_frame = normed_r.iloc[ix_offset - lookback_window:ix_offset, :-1]\n",
    "    print(\"time index and covariance matrix shape\", ix, ret_frame.shape)\n",
    "    cov_matricies[ik, :, :] = ret_frame.cov()\n",
    "    ik += 1\n",
    "\n",
    "print ('Done')\n",
    "# the last covariance matrix determines the absorption ratio\n",
    "lin_ae = LinearAutoEncoder(n_inputs=num_assets, n_codings=200)\n",
    "np.array([cov_matricies[-1, :, :]]).shape\n",
    "lin_codings, test_absorp_ratio = lin_ae.train(cov_matricies[ : int((2/3)*num_samples), :, :],\n",
    "                                                np.array([cov_matricies[-1, :, :]]),\n",
    "                                                n_epochs=10, \n",
    "                                                batch_size=5)\n",
    "lin_codings, in_sample_absorp_ratio = lin_ae.absorption_ratio(np.array([cov_matricies[0, :, :]]))\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong, please have a look at the reponse of the grader\n",
      "-------------------------\n",
      "{\"errorCode\":\"invalidEmailOrToken\",\"message\":\"Invalid email or token.\",\"details\":null}\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39359208195496026, 0.39359208195496026]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_2=[test_absorp_ratio, in_sample_absorp_ratio]\n",
    "try:\n",
    "    part2 = \" \".join(map(repr, part_2))\n",
    "except TypeError:\n",
    "    part2 = repr(part_2)\n",
    "submissions[all_parts[1]]=part2\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:2],all_parts,submissions)\n",
    "[test_absorp_ratio, in_sample_absorp_ratio]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half-life = 252\n",
      "Lookback window = 504\n",
      "Step size = 1\n",
      "Variance Threshold = 0\n",
      "Number of stocks = 418\n",
      "Number of principal components = 83\n"
     ]
    }
   ],
   "source": [
    "stock_tickers = asset_returns.columns.values[:-1]\n",
    "assert 'SPX' not in stock_tickers, \"By accident included SPX index\"\n",
    "\n",
    "half_life = 252             # in (days)\n",
    "'''\n",
    "lookback_window is the number of trading days used to calculate historical \n",
    "statistics in a moving window fashion. It ensures that each analysis step \n",
    "has a consistent and sufficient amount of historical data. This window moves\n",
    "forward through the dataset by the specified step_size in each iteration, \n",
    "allowing for a dynamic and rolling analysis.\n",
    "'''\n",
    "lookback_window = 252 * 2   # in (days) the number of trading days over which you calculate statistics (e.g., covariance matrix) for each iteration of the analysis. It defines the size of the moving window used to analyze historical data.\n",
    "num_assets = len(stock_tickers)\n",
    "step_size = 1          # days : 5 - weekly, 21 - monthly, 63 - quarterly\n",
    "\n",
    "# require of that much variance to be explained. How many components are needed?\n",
    "var_threshold = 0.8     \n",
    "\n",
    "# fix 20% of principal components for absorption ratio calculation. How much variance do they explain?\n",
    "absorb_comp = int((1 / 5) * num_assets)  \n",
    "\n",
    "print('Half-life = %d' % half_life)\n",
    "print('Lookback window = %d' % lookback_window)\n",
    "print('Step size = %d' % step_size)\n",
    "print('Variance Threshold = %d' % var_threshold)\n",
    "print('Number of stocks = %d' % num_assets)\n",
    "print('Number of principal components = %d' % absorb_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    }
   ],
   "source": [
    "# indexes date on which to compute PCA\n",
    "'''\n",
    "The days_offset is a predefined number of trading days used to skip the \n",
    "initial part of the dataset, ensuring that the analysis starts after a \n",
    "sufficient amount of historical data has been accumulated. This helps \n",
    "stabilize the analysis, avoid lookback bias, and ensure consistency in \n",
    "the rolling or moving window \n",
    "'''\n",
    "days_offset = 4 * 252 # 4 Years\n",
    "#The num_days variable in your code defines the total number of trading days\n",
    "#over which the analysis will be conducted.\n",
    "num_days = 6 * 252 + days_offset # Calculates num_days as 6 years plus days_offset.\n",
    "#Generates an index (pca_ts_index) for which PCA will be computed. \n",
    "#It likely selects specific dates or intervals from normed_r for PCA analysis.\n",
    "#Creates a list of dates corresponding to specific indices where PCA and \n",
    "# absorption ratio calculations will be performed.\n",
    "pca_ts_index = normed_r.index[list(range(lookback_window + days_offset, min(num_days, len(normed_r)), step_size))]\n",
    "# pca_ts_index = normed_r.index[list(range(2*252 + 4*252, min(num_days, len(normed_r)), 21))]\n",
    "# This ensures that the analysis is conducted at regular intervals (defined by step_size) starting after an initial period (defined by lookback_window + days_offset) \n",
    "# and continuing up to the specified end point (min(num_days, len(normed_r))).\n",
    "\n",
    "# allocate arrays for storing absorption ratio\n",
    "#These will store PCA components, absorption ratios, and ratios computed by the Auto-Encoder, respectively.\n",
    "pca_components = np.array([np.nan]*len(pca_ts_index))\n",
    "absorp_ratio = np.array([np.nan]*len(pca_ts_index))\n",
    "lae_ar = np.array([np.nan]*len(pca_ts_index))  # absorption ratio computed by Auto-Encoder \n",
    "\n",
    "# keep track of covariance matricies as we would need them for training Auto-Encoder\n",
    "buf_size = 5\n",
    "cov_matricies = np.zeros((buf_size, num_assets, num_assets))\n",
    "\n",
    "#Computes exponential weighting probabilities,  for weighting historical data in calculations. \n",
    "#This function might  a decay mechanism for older data.\n",
    "exp_probs = exponent_weighting(lookback_window, half_life)\n",
    "assert 'SPX' not in normed_r.iloc[:lookback_window, :-1].columns.values, \"By accident included SPX index\"\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "- on each loop iteration: \n",
    "    - fit PCA to **cov_mat**\n",
    "    - use fitted pca model to pass values to absorption_ratio(). The result of absorption ratio calculation goes into **absorp_ratio**\n",
    "    - compute number of principal components it takes to explain at least **var_threshold** of variance. The result of this calculation goes into **pca_components** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainging AE 2006-02-07 00:00:00\n",
      "Linear Auto-Encoder: variance explained: 0.22\n",
      "Done training linear auto-encoder\n",
      "Trainging AE 2007-02-09 00:00:00\n",
      "Linear Auto-Encoder: variance explained: 0.22\n",
      "Done training linear auto-encoder\n",
      "Trainging AE 2008-02-11 00:00:00\n",
      "Linear Auto-Encoder: variance explained: 0.25\n",
      "Done training linear auto-encoder\n",
      "Trainging AE 2009-02-10 00:00:00\n",
      "Linear Auto-Encoder: variance explained: 0.18\n",
      "Done training linear auto-encoder\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code  calculates the absorption ratio and PCA components over time \n",
    "using a moving window of returns. It does this for both exponentially weighted \n",
    "returns and equally weighted returns. \n",
    "Additionally, it trains a linear autoencoder to compute the absorption \n",
    "ratio and compare it with the PCA-based absorption ratio.\n",
    "'''\n",
    "# run the main loop computing PCA and absorption at each step using moving window of returns  \n",
    "# run this loop using both exponentially weighted returns and equally weighted returns\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "ik = 0 # loop counter.\n",
    "use_ewm = False #determines whether to use exponentially weighted returns\n",
    "lin_ae = None\n",
    "time_start = time.time() # records the start time for performance measurement.\n",
    "'''\n",
    "Starting Index: lookback_window (252 * 2) + days_offset (4 * 252)\n",
    "Ending Index: min(num_days(6 * 252 + days_offset), len(normed_r))\n",
    "Step Size: 21\n",
    "'''\n",
    "for ix in range(lookback_window + days_offset, min(num_days, len(normed_r)), step_size):\n",
    "    '''\n",
    "    extracts a fixed window of returns from the normalized returns DataFrame \n",
    "    (normed_r). This window is used for calculating the covariance matrix, \n",
    "    which is essential for PCA and other analyses. \n",
    "    '''\n",
    "    ret_frame = normed_r.iloc[ix - lookback_window:ix, :-1]  # fixed window\n",
    "    # ret_frame = normed_r.iloc[:ix, :-1]  # ever-growing window\n",
    "    #applying exponential weighting to the returns data if the use_ewm flag is set to True\n",
    "    if use_ewm:\n",
    "        ret_frame = (ret_frame.T * exp_probs).T\n",
    "    \n",
    "    cov_mat = ret_frame.cov() # calculates the covariance matrix of the returns data in ret_frame\n",
    "    circ_idx = ik % buf_size # calculates the circular index circ_idx based on the current iteration index ik and the buffer size buf_size.\n",
    "    #This technique allows efficient management of historical data in a fixed-size buffer, commonly used in time-series analysis and streaming data applications.\n",
    "    cov_matricies[circ_idx, :, :] = cov_mat.values\n",
    "    # pdates a specific position in a 3D NumPy array (cov_matricies) with a newly calculated covariance matrix (cov_mat). This approach efficiently manages and analyzes historical covariance data in a circular buffer setup, supporting various time-series analytical techniques in finance and statistics.\n",
    "    \n",
    "    '''\n",
    "    The following code block manages the computation of PCA and absorption \n",
    "    ratio based on the covariance matrix cov_mat. It ensures periodic updates \n",
    "    and continuous tracking of relevant metrics for time-series analysis, \n",
    "    such as financial data or other sequential data streams\n",
    "    '''\n",
    "    if ik == 0 or ik % 21 == 0:\n",
    "        ### START CODE HERE ### (â‰ˆ 4-5 lines of code)\n",
    "        ### fit PCA, compute absorption ratio by calling absorption_ratio()\n",
    "        ### store result into pca_components for grading\n",
    "        pca = PCA().fit(cov_mat)\n",
    "        absorp_ratio[ik] = absorption_ratio(pca.explained_variance_, absorb_comp)\n",
    "        var_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "        #dynamically determines the number of principal components based on a variance explained threshold var_threshold. It facilitates effective dimensionality reduction and variance capture in PCA-based analyses, ensuring optimal feature selection for subsequent modeling or interpretation.\n",
    "        pca_components[ik] = np.where(np.logical_not(var_explained < var_threshold))[0][0] + 1        \n",
    "        ### END CODE HERE ###\n",
    "    else: # ensures that absorp_ratio and pca_components arrays are updated based on the previous values when the condition for new PCA calculation and absorption ratio update is not met. This approach helps maintain data continuity and consistency in your iterative process, especially useful for time-series analysis and tracking of dynamic metrics like PCA components and absorption ratios over time.\n",
    "        absorp_ratio[ik] = absorp_ratio[ik-1] \n",
    "        pca_components[ik] = pca_components[ik-1]\n",
    "    \n",
    "    if ik == 0 or ik % 252 == 0:    # ensures that the lin_ae object is properly managed and reset at the beginning (ik == 0) and periodically (every 252 iterations) during its operation. This practice helps in maintaining the integrity and efficiency of lin_ae throughout its usage, especially in contexts involving iterative computations or long-term data processing tasks.\n",
    "        if lin_ae is not None:\n",
    "            lin_ae.destroy()\n",
    "\n",
    "        print('Trainging AE', normed_r.index[ix])\n",
    "        lin_ae = LinearAutoEncoder(cov_mat.shape[0], absorb_comp) #is crucial for setting up an instance of LinearAutoEncoder with specific parameters. It facilitates the use of the LinearAutoEncoder functionality throughout your code, allowing you to encapsulate operations related to linear autoencoding within the instantiated object lin_ae. This approach supports modularity, reusability, and clear separation of concerns in your program.\n",
    "        # itegrates the training process of a linear autoencoder (lin_ae) into your workflow, utilizing historical covariance matrices (cov_matricies) and current covariance matrix (cov_mat). This operation facilitates dimensionality reduction and absorption ratio computation, essential for analyzing and understanding complex datasets efficiently.\n",
    "        lin_codings, lae_ar[ik] = lin_ae.train(cov_matricies[:circ_idx + 1, :, :], np.array([cov_mat.values]),batch_size=2)\n",
    "    else: #Computes absorption ratio using lin_ae in iterations other than the initialization/reset phases (ik != 0 and ik % 252 != 0)., itgrates the absorption ratio computation into your workflow using a linear autoencoder (lin_ae). This operation helps quantify the impact or absorption of underlying components (like principal components in PCA) within your dataset, facilitating deeper insights and analysis in applications such as financial modeling or dimensionality reduction tasks.\n",
    "        lin_codings, lae_ar[ik] = lin_ae.absorption_ratio(np.array([cov_mat.values]))\n",
    "\n",
    "    ik += 1\n",
    "    \n",
    "print ('Absorption Ratio done! Time elapsed: {} seconds'.format(time.time() - time_start))    \n",
    "ts_pca_components = pd.Series(pca_components, index=pca_ts_index)\n",
    "ts_absorb_ratio = pd.Series(absorp_ratio, index=pca_ts_index)\n",
    "ts_lae_absorb_ratio = pd.Series(lae_ar, index=pca_ts_index)\n",
    "#This organization facilitates easy manipulation, analysis, and visualization of time series data within your Python environment using pandas' powerful functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_delta_data = ar_delta[251:]\n",
    "# ou can easily generate a line plot to visualize how the absorption ratio computed via PCA changes over time. This visualization aids in understanding the relative influence or absorption of principal components in your dataset across different periods, supporting deeper analysis and decision-making processes.\n",
    "ts_absorb_ratio.plot(figsize=(12,6), title='Absorption Ratio via PCA', linewidth=3)\n",
    "plt.savefig(\"Absorption_Ratio_SPX.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a line plot to visualize how the absorption ratio computed via an auto-encoder changes over time. This visualization helps in understanding the relative influence or absorption of the auto-encoder's learned representations in your dataset across different periods, facilitating deeper analysis and interpretation of model performance.\n",
    "ts_lae_absorb_ratio.plot(figsize=(12,6), title='Absorption Ratio via Auto-Encoder', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, a larger absorption ratio via PCA suggests that PCA may be more suitable for your dataset in terms of efficiently capturing variance with fewer components, leveraging its linear dimensionality reduction capabilities. \n",
    "In conclusion, while PCA stabilizing its absorption ratio and an autoencoder's absorption ratio dropping suggest different modeling outcomes, understanding these behaviors helps in choosing the appropriate method based on the dataset's characteristics, modeling goals, and desired outcomes in variance explanation and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having computed daily (this means the step size is 1) Absorption Ratio times series, we further follow M. Kritzman to make use of AR to define yet another measure: AR Delta. In particular:\n",
    "$$ AR\\delta = \\frac{AR_{15d} - AR_{1y}}{ AR\\sigma_{1y}}$$\n",
    "We use  $AR\\delta$ to build simple portfolio trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# following Kritzman and computing AR_delta = (15d_AR -1yr_AR) / sigma_AR\n",
    "'''\n",
    "This code will generate a plot with three lines representing AR_delta, \n",
    "AR_1yr, and AR_15d over time, providing a visual overview of how the \n",
    "absorption ratio behaves and changes relative to its historical averages \n",
    "and volatility. Adjust the parameters (252 for 1-year rolling mean, 15 for \n",
    "15-day rolling mean) based on your specific data and analysis needs.\n",
    "'''\n",
    "ts_ar = ts_absorb_ratio\n",
    "ar_mean_1yr = ts_ar.rolling(252).mean()\n",
    "ar_mean_15d = ts_ar.rolling(15).mean()\n",
    "ar_sd_1yr = ts_ar.rolling(252).std()\n",
    "# An AR Delta (Absorption Ratio Delta) trading strategy uses the absorption ratio and its changes over time to make trading decisions\n",
    "ar_delta = (ar_mean_15d - ar_mean_1yr) / ar_sd_1yr    # standardized shift in absorption ratio\n",
    "\n",
    "df_plot = pd.DataFrame({'AR_delta': ar_delta.values, 'AR_1yr': ar_mean_1yr.values, 'AR_15d': ar_mean_15d.values}, \n",
    "                       index=ts_ar.index)\n",
    "df_plot = df_plot.dropna()\n",
    "if df_plot.shape[0] > 0:\n",
    "    df_plot.plot(figsize=(12, 6), title='Absorption Ratio Delta', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 (AR Delta Trading Strategy)\n",
    "\n",
    "An AR Delta (Absorption Ratio Delta) trading strategy uses the absorption ratio and its changes over time to make trading decisions. This strategy can be particularly useful for identifying shifts in market conditions, potential turning points, and periods of increased market risk. This approach provides a framework for using AR Delta to inform trading decisions, leveraging changes in market absorption ratios to identify potentially profitable opportunities.\n",
    "By dynamically adjusting the portfolio weights based on the AR delta, this strategy aims to capitalize on favorable market conditions for equities while reducing exposure during periods of increased market risk, as indicated by changes in the absorption ratio.\n",
    "\n",
    "**Instructions:** Implement get_weight() function\n",
    "\n",
    "The AR Delta trading strategy forms a portfolio of EQ and FI, following these simple rules:\n",
    "\n",
    "* __$ -1\\sigma < AR < +1\\sigma $__\t 50 / 50 weights for EQ / FI\n",
    "* __$ AR > +1\\sigma $__\t             0 / 100 weights for EQ / FI\n",
    "* __$ AR < -1\\sigma $__\t             100 / 0 weights for EQ / FI\n",
    "\n",
    "Here we compute AR Delta strategy weights using data from the same data set. As expected, the average number of trades per year is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: get_weight\n",
    "def get_weight(ar_delta):\n",
    "    '''\n",
    "    Calculate EQ / FI portfolio weights based on Absorption Ratio delta\n",
    "    Calculating portfolio weights for Equity (EQ) and Fixed Income (FI) based\n",
    "    on Absorption Ratio (AR) delta involves using the AR delta as a signal \n",
    "    to dynamically adjust the allocation between these asset classes. \n",
    "    The idea is to increase exposure to equities when the AR delta indicates \n",
    "    a favorable market condition and to shift towards fixed income when the \n",
    "    AR delta suggests increased market risk or unfavorable conditions\n",
    "    \n",
    "    Arguments:\n",
    "    ar_delta -- Absorption Ratio delta\n",
    "    \n",
    "    Return: \n",
    "        wgts -- a vector of portfolio weights\n",
    "    \n",
    "    The function returns portfolio weights based on the value of ar_delta. Here's the adjusted implementation, using this simple rule-based approach:\n",
    "\n",
    "    If ar_delta > 1: Full allocation to equities.\n",
    "    If ar_delta < -1: Full allocation to fixed income.\n",
    "    Otherwise: Equal allocation to both equities and fixed income.\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 6 lines of code)\n",
    "    if ar_delta > 1:\n",
    "        return [0., 1.]\n",
    "    elif ar_delta < -1:\n",
    "        return [1., 0.]\n",
    "    \n",
    "    wgts = [0.5, 0.5]\n",
    "    return wgts\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This approach allows you to track how often your portfolio is rebalanced \n",
    "# based on the absorption ratio delta and analyze the stability and \n",
    "# responsiveness of your trading strategy.\n",
    "### GRADED PART (DO NOT EDIT) ###\n",
    "# is extracting a portion of the ar_delta series, specifically starting from the 252nd element. This means the first 251 values are excluded from ar_delta_data. \n",
    "#The first 251 values in ar_delta are likely NaN (Not a Number) because there isnâ€™t enough data to calculate a rolling mean or standard deviation for the first 252 days.\n",
    "ar_delta_data = ar_delta[251:]\n",
    "\n",
    "rebal_dates = np.zeros(len(ar_delta_data)) # This array is used to keep track of the rebalancing events in your trading strategy\n",
    "wgts = pd.DataFrame(data=np.zeros((len(ar_delta_data.index), 2)), index=ar_delta_data.index, columns=('EQ', 'FI')) # initializes the portfolio weights based on the first value of the ar_delta_data series\n",
    "\n",
    "prtf_wgts = get_weight(ar_delta_data.values[0]) #nitializes the portfolio weights based on the first value of the ar_delta_data series\n",
    "wgts.iloc[0, :] = prtf_wgts \n",
    "'''\n",
    "wgts.iloc[0, :] = prtf_wgts sets the initial portfolio weights at the first index of the DataFrame wgts.\n",
    "wgts is a DataFrame with two columns ('EQ' and 'FI') representing the weights for equities and fixed income, respectively.\n",
    "prtf_wgts is a list containing the initial portfolio weights for equities and fixed income, as determined by the get_weight function based on the first value of ar_delta_data.\n",
    "'''\n",
    "\n",
    "for ix in range(1, len(ar_delta_data)): # updates the portfolio weights, and checks for rebalancing events. \n",
    "    prtf_wgts = get_weight(ar_delta_data.values[ix])\n",
    "    wgts.iloc[ix, :] = prtf_wgts\n",
    "    if wgts.iloc[ix-1, :][0] != prtf_wgts[0]: # compares the current weights with the previous weights to detect any changes. If there is a change, it updates the rebalancing date\n",
    "        # Detects changes in portfolio allocation, marking rebalancing dates.\n",
    "        prtf_wgts = wgts.iloc[ix, :]\n",
    "        rebal_dates[ix] = 1\n",
    "\n",
    "#reating the ts_rebal_dates series and using it to analyze rebalancing events is crucial for understanding the frequency and timing of adjustments in your portfolio based on the absorption ratio delta.\n",
    "ts_rebal_dates = pd.Series(rebal_dates, index=ar_delta_data.index)\n",
    "#quantify and analyze the annual frequency of rebalancing events within your trading strategy. This information is crucial for evaluating the strategy's performance, effectiveness, and operational characteristics over different periods.\n",
    "ts_trades_per_year = ts_rebal_dates.groupby([ts_rebal_dates.index.year]).sum()\n",
    "print('Average number of trades per year %.2f' % ts_trades_per_year.mean())\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "np.random.seed(42)\n",
    "wgts_test = wgts.as_matrix()\n",
    "idx_row = np.random.randint(low=0, high=wgts_test.shape[0], size=100)\n",
    "np.random.seed(42)\n",
    "idx_col = np.random.randint(low=0, high=wgts_test.shape[1], size=100)\n",
    "\n",
    "# grading\n",
    "part_3=list(wgts_test[idx_row, idx_col])\n",
    "try:\n",
    "    part3 = \" \".join(map(repr, part_3))\n",
    "except TypeError:\n",
    "    part3 = repr(part_3)\n",
    "submissions[all_parts[2]]=part3\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:3],all_parts,submissions)\n",
    "\n",
    "wgts_test[idx_row, idx_col]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that weights have been determined, run the re-balancing strategy using time series of returns and compute\n",
    " - sharpe of the strategy\n",
    " - strategy annualized return\n",
    " - strategy annualized volatility\n",
    "\n",
    "Contrast this with 50 / 50 Equity / Fixed Income ETF strategy performance using the same performance metrics. Use VTI as Equity and AGG as Fixed Income assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etf_r= pd.read_csv('/home/jovyan/work/readonly/pca_hw5_etf_returns.csv',\n",
    "                     date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),\n",
    "                     index_col = 0)\n",
    "etf_prices = pd.read_csv('/home/jovyan/work/readonly/millenials_portfolio_etfs.csv',\n",
    "                         date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),\n",
    "                         index_col = 0)\n",
    "etf_returns = etf_prices.pct_change(periods=1)\n",
    "etf_returns = etf_returns.iloc[1450:, :]\n",
    "etf_r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 (Calculate performance of backtested strategy)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Implement function backtest_strategy which given a DataFrame of strategy weights and a DataFrame asset returns annualized return, volatility and Sharpe ratio of a strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backtest_strategy\n",
    "def backtest_strategy(strat_wgts, asset_returns, periods_per_year = 252):\n",
    "    '''\n",
    "    Calculate portfolio returns and return portfolio strategy performance\n",
    "    Arguments:\n",
    "    \n",
    "    strat_wgts -- pandas.DataFrame of weights of the assets\n",
    "    asset_returns -- pandas.DataFrame of asset returns\n",
    "    periods_per_year -- number of return observations per year\n",
    "    \n",
    "    Return: \n",
    "        (ann_ret, ann_vol, sharpe) -- a tuple of (annualized return, annualized volatility, sharpe ratio)\n",
    "    '''\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ 10 lines of code)\n",
    "    df_ = strat_wgts.join(asset_returns)\n",
    "    df_['EQ_returns'] = df_['EQ'] * df_['VTI']\n",
    "    df_['FI_returns'] = df_['FI'] * df_['AGG']\n",
    "    annualized_return = (df_['EQ_returns'].mean() + df_['FI_returns'].mean()) * periods_per_year\n",
    "    annualized_vol = (df_['EQ_returns'] + df_['FI_returns']).std() * np.sqrt(periods_per_year)\n",
    "    sharpe_ratio = annualized_return / annualized_vol\n",
    "    return annualized_return, annualized_vol, sharpe_ratio\n",
    "    # return 0., 0., 1. # annualized return,  annualized volatility,  sharp ratio\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "ann_ret, ann_vol, sharpe = backtest_strategy(wgts, etf_r)\n",
    "print('Absorption Ratio strategy:', ann_ret, ann_vol, sharpe)\n",
    "\n",
    "eq_wgts = wgts.copy()\n",
    "eq_wgts.iloc[:, ] = 0.5\n",
    "ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt = backtest_strategy(eq_wgts, etf_r)\n",
    "print('Equally weighted:', ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt)\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_4=[ann_ret, ann_vol, sharpe, ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt]\n",
    "try:\n",
    "    part4 = \" \".join(map(repr, part_4))\n",
    "except TypeError:\n",
    "    part3 = repr(part_4)\n",
    "submissions[all_parts[3]]=part4\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:4],all_parts,submissions)\n",
    "[ann_ret, ann_vol, sharpe, ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "coursera": {
   "course_slug": "machine-learning-in-finance"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
